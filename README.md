COMP0235 – Distributed Protein Analysis Pipeline

UCL Computer Science — Distributed Systems Coursework
Last updated: 02 Dec 2025

This project implements a fully automated distributed protein-processing pipeline. Terraform provisions the Harvester VMs, Ansible configures the environment, Redis acts as the message broker, and Celery executes distributed tasks across five worker machines. All code, task files, helper scripts, and datasets are stored on a shared NFS directory so every worker runs in an identical environment.

Project Structure

ansible/ Ansible configuration (all.yaml, host.yaml, workers.yaml, roles/)
build_cluster/ Terraform VM provisioning and dynamic inventory generation
shared/ (NFS) Code, Celery tasks, helper scripts, datasets, experiment IDs
scripts/ Celery tasks plus helperScripts containing non-Celery utilities
pipeline/ Distributed pipeline controller (now integrated into tasks.py)
logs/ Worker and pipeline runtime logs

System Overview

The entire cluster can be built and configured using:

terraform apply
ansible-playbook full.yaml

Workflow:

Terraform creates the host and worker virtual machines.

generate_inventory.py reads Terraform outputs and generates a dynamic Ansible inventory.

Ansible configures all machines, mounts the NFS share, installs required software, deploys Celery, and downloads all required datasets.

Celery workers start under systemd and automatically load task definitions from the shared NFS directory.

This creates a fully reproducible, self-contained distributed system.

Ansible Configuration
GitLab Integration

The host clones the coursework Git repository using variables defined inside host.yaml:

gitrepourl
gituser
gittoken

HTTPS-based cloning avoids distributing SSH keys and ensures reproducible pulls from GitLab.

Shared NFS Directory

The host exports:

/shared/almalinux

Workers mount this directory so all machines share the same:

scripts/
celery/ (Celery tasks)
helperScripts/ (non-Celery utilities: results_parser.py, select_ids.py)
src/
data/
dataset/uniprot/
dataset/pdb70/

This guarantees that every Celery worker uses the same codebase and datasets.

Role Reorganisation (Dec 2025)

The Ansible configuration has been fully refactored for clarity:

host_storage → host_configure_nfs
host_redis → host_configure_redis
worker_storage → worker_configure_nfs
worker_celery → worker_configure_celery
worker_python → worker_python_dependencies

Deprecated or unused roles (seconddisk, cnc, old host_pipeline) have been moved to roles/archive/.

NFS Population Pipeline

A new high-level role, host_nfs_populator, handles population of the shared directory:

host_clone_git
host_download_datasets
host_install_s4pred
host_install_hhsuite

Only the first two are currently implemented.

Code Synchronisation

Before copying the fresh GitLab clone into /shared/almalinux/, the host removes the old:

src/
scripts/
data/

This prevents stale files staying in the shared directory.
A future improvement will use ansible.posix.synchronize(delete=yes) for fully mirrored updates.

Dataset Download and Layout

Datasets are now placed into named subdirectories:

dataset/uniprot/uniprot_dataset.fasta.gz
dataset/pdb70/pdb70_dataset.tar.gz (plus extracted files)

The playbook:

checks whether the dataset exists

downloads only if missing

uses temporary directories

extracts pdb70 automatically

deletes all temporary files afterward

This prevents unnecessary downloads and avoids corrupted partial files.

Dynamic Celery Configuration

Celery configuration (celeryconfig.py) is generated by a Jinja2 template which sets:

redis://{{groups["host"][0]}}:6379/0

so Celery always targets the correct Redis host even if Terraform generates new IPs.

A systemd service manages the workers and automatically restarts them when configuration changes.

Current Status (as of 02 Dec 2025)

Terraform cluster builds successfully.

Dynamic inventory generation is stable.

Ansible roles have been fully reorganised for clarity.

host_nfs_populator handles Git syncing and dataset management.

NFS layout is consistent and idempotent across all machines.

UniProt and pdb70 datasets are downloaded and extracted automatically.

Celery workers run under systemd and load tasks from the shared folder.

All pipeline stages from pipeline_script.py have been rewritten as Celery tasks in tasks.py.

helperScripts contains non-Celery tools (results_parser.py, select_ids.py).

Shared NFS folder updates cleanly on every Ansible run.

Cluster-wide checks confirm identical task lists on all workers.

Summary

The cluster is now cleanly structured, fully automated, and nearly ready for a complete end-to-end pipeline test. Terraform builds the machines, Ansible configures everything including GitLab cloning and dataset management, and Celery distributes protein-processing tasks reliably across all workers using Redis for coordination.
